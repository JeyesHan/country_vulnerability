from __future__ import print_function,division
import torch
from torch.autograd import Variable
from torch.utils.data import Dataset,DataLoader
from torchvision import transforms,utils

from dataprocess.transform import New_ToTensor
from dataprocess.load_data import Countrydataset
from test_method import test
from modelvilization.loss_decling_graph import show_loss
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--cuda',           action='store_true',    default=True,   help='using GPU')
parser.add_argument('--epochs',         type=int,               default=40,     help= 'epoch number')
parser.add_argument('--batch_size',     type=int,               default=32,     help='batch size')
parser.add_argument('--learning_rate',  type=float,             default=2e-3,   help='learning rate')
parser.add_argument('--H',              type=int,               default=100,    help='hidden neurons')
parser.add_argument('--show',           action='store_true',    default=True,   help='show loss decline')

args = parser.parse_args()

data_transform = transforms.Compose([
    New_ToTensor(),
])

#load dataset
countrydataset_train = Countrydataset('data/fsi2017.csv',
                                transform=data_transform)

countrydataset_test = Countrydataset('data/fsi2016.csv',
                                transform=data_transform)

training_data_loader = DataLoader(countrydataset_train,
                         batch_size=args.batch_size,
                         shuffle=True,
                         num_workers=1,
                        drop_last=False)

testing_data_loader = DataLoader(countrydataset_test,
                         batch_size=args.batch_size,
                         shuffle=True,
                         num_workers=1,
                        drop_last=False)

#setup the network
model = torch.nn.Sequential(
    torch.nn.Linear(12,args.H),
    torch.nn.ReLU(),
    torch.nn.Linear(args.H,1)
).cuda()

#loss function
loss_fn = torch.nn.MSELoss(size_average=True)

#optimizer method
optimizer = torch.optim.Adam(model.parameters(),lr=args.learning_rate)

epoch_losses = []
for epoch in range(args.epochs):
    print ("====>Epoch:{}/{}<====".format(epoch+1,args.epochs))
    epoch_loss = 0.0
    for batch_number,sample_batched in enumerate(training_data_loader):
        country_ranks,country_aspects = sample_batched['country_ranks'],sample_batched['country_aspects']
        country_ranks,country_aspects = Variable(country_ranks,requires_grad = False).float().cuda(),\
                                        Variable(country_aspects,requires_grad = False).float().cuda()

        country_ranks_predict = model(country_aspects)
        loss = loss_fn(country_ranks_predict,country_ranks)
        epoch_loss = epoch_loss + loss.data[0]
        print("batch_number:{}".format(batch_number)+"  loss:{}".format(loss.data[0]))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    epoch_losses.append(epoch_loss/batch_number)
    if (epoch+1) % 10 == 0:
        test(testing_data_loader, model, loss_fn, args)

#show training loss decline graph
if args.show:
    show_loss(epoch_losses)





